---
title: "Sampling"
output: html_document
---

```{r, message=FALSE}
library(here)
library(tidyverse)
```


# 1. Sampling in R

- Sampling distribution: Distribution of all the possible values of a sample statistics for a given sample size selected from a population

## 1.1 Sampling distribution of the means
- Distribution of means $\hat{x}$ created by taking many repeated random samples of size n. 
- The mean of the sampling distribution is the population mean  $\mu_{\hat{x}}$. 
- The standard deviation of this distribution of the sample means is known as the standard error (SE) or the SE of the mean.

$$SE_{\text{of the mean}} = \sigma_{\hat{x}} = \frac{\sigma}{\sqrt{n}}$$
-> SE decreases as the sample size increases.


# 2. Central Limit Theorem

CLT implies that a population mean can be estimated by obtaining a large enough representative sample from the larger population. 

- When sampling from a population with mean $\mu$ and standard deviation $\sigma$, the sampling distribution of the sample mean will tend to a normal distribution with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$ as the sample size becomes large ($n > 30$).


# 3. Sample size

Calculation:

$$n = (\frac{Z_{\alpha/2}*\sigma}{E})^2$$
Where:
- $E$ = Margin of error that we are willing to accept
- $1 - \alpha$ = the probability that the magnitude of that error will not exceed

```{r}
# Create the function to determine the sample size
samp_size <- function(z, E, sigma){
  out <- ((z * sigma) / E)^2
  out
}
```

```{r}
# Create our variables
alpha <- 0.05 # for a 95% CI, alpha is 5%
sigma <- 0.03 # the standard deviation of the chain of shops
E <- 0.01 # The expected effect size (MoE)
z <- qnorm(alpha/2, lower.tail = FALSE)

# Example
n <- samp_size(z = z, E = E, sigma = sigma)
print(ceiling(n)) # round up to the next integer
```

# 4. Confidence intervals
## 4.1 Confidence intervals

Allow you to gauge the level of confidence you can attach to a point estimate like mean or proportion.

- Is the population standard deviation known, use z-score
- Is the population standard deviation unknown, use t-score and substitude with the sample standard deviation $s$. Also $n-1$.

## 4.1.1 Confidence interval for a population mean

$$\text{CI (SD known) = }\hat{x} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$$
- Standard error: $\frac{\sigma}{\sqrt{n}}$
- Z = $\frac{\hat{X}-\mu}{\sigma / \sqrt{n}}$

### Example Z-Score

```{r}
# Example
alpha = 0.05 # 1 - alpha of 95%
Z = 1.96 # Z-score
x_hat = 172.05 # Point estimate xhat
sd = 6.35 # Standard deviation
n = 10 # Sample
# Calculation
margin_of_error = Z * (sd/sqrt(n))
lower_limit = x_hat - margin_of_error
upper_limit = x_hat + margin_of_error
glue::glue("A 95% confidence interval (known SD) 
           limit goes from {round(lower_limit,2)}cm to {round(upper_limit,2)}cm for the self-reported height of men.")
```

### Example t-score

$$\text{CI (SD unknwown) = }\hat{x} \pm t_{n-1,\alpha/2}*\frac{s}{n-1}$$

We use the sample metrics as above:
- X_hat of 172.05
- n = 10
- Significance level of 95% (1 - alpha / 2 in each tail)
- BUT: Sample standard deviation now: 6.06

```{r}
# Calcualte the t-distribution and new margin of error
sd_sample = 6.06
t_score = qt(0.05/2, 9, lower.tail=FALSE) # alpha / 2 for each tail , n - 1 = 9
margin_of_error_new = t_score * (sd_sample/sqrt(n))
lower_limit_new = x_hat - margin_of_error_new
upper_limit_new = x_hat + margin_of_error_new
# Result
glue::glue("A 95% confidence interval (unknwon SD) 
           limit goes from {round(lower_limit_new,2)}cm to {round(upper_limit_new,2)}cm for the self-reported height of men.")
```

## 4.2 Confidence interval for a Population Proportion
Examples:
- What proportion of Australians are willing to take out a mortgage for up to five times their annual income?
- What percentage of children aged 8-11 have their own smartphone?
- What percentage of the population owened a car before they were 20?
- Standard error of the point estimate for p = $\hat{p} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$
- Sample proption $\hat{p} = \frac{\text{positive responses}}{\text{all responses}}$ 

$$\text{CI_proportion} = \hat{p} \pm Z * \sqrt{\frac{\hat{p}*(1-\hat{p})}{n}}$$

```{r}
# Example
# Imagine we are interested in the proportion of adults in Melbourne who watch movies on a tablet. A random survey was conducted which found that of the 400 people surveyed, 342 responded yes – they watch movies on a tablet. Compute a confidence interval estimate using a 95 per cent confidence level for the true proportion of adults who watch movies on a tablet. They either watch movies on a tablet or they don’t, so we know we are dealing with a binomial distribution where n = 400. Here then are the steps in calculating the confidence interval.

# Values
n <- 400
phat <- 342 / n # Sample proportion
SE_phat <- sqrt((phat * (1 - phat)) / n) # Standard Error
Z <- qnorm(0.05/2, 0, 1, lower.tail=FALSE) # Z-Score
# CI propotion
lower_limit_pro <- phat - Z * SE_phat
upper_limit_pro <- phat + Z * SE_phat
glue::glue("We estimate with 95% confidence that the true proportion of adults in Melbourne who watch movies on their tablets is between {round(lower_limit_pro,2)}% and {round(upper_limit_pro,2)}%.")
```

### Sample size of proportions

$$n = (\frac{Z_{\alpha/2}^2*\sigma^2}{ME^2})$$
where:
$\sigma^2 = p(1-p)$

```{r}
# Example for sample means
# Suppose you want to derive a 95% confidence interval for the average number of hours per week that 8–11-year-old kids spend watching TV, where the margin of error is around 1 hour. You conduct a small pilot study and obtain a sample standard deviation of 3.5 hours.

n = ((1.96^2)*(3.5)^2)/1^2
ceiling(n)
```

```{r}
# Example for sample proportion
# Suppose you want to derive a 95% confidence interval for the proportion of 8–11-year-old kids who are allowed to take their smartphone to bed, where the margin of error is 10%. You conduct a small pilot study and obtain an initial estimate of 40% being allowed to take their smartphone to bed.

p = 0.4
n = ((1.96^2)*p*(1-p))/0.1^2
ceiling(n)
```


# 5. Hypothesis testing
## 5.1 Fundamentals of hypothesis testing

Six Step procedure:
1. Frame your competing hypotheses in terms of parameter of interest
2. Assume the null hypothesis is true $H_0$
3. Choose your significance level $\alpha$
4. Calculate the observed value of the test statistic from your random sample
5. Calculate the p-value and compare it with your significance level
6. Reject your null hypothesis if and only if the p-value < significance level $\alpha$

Which hypothesis test to use? For Z-test you need:
- Sample size greater than 30
- Known population standard deviation
-> Is one not fulfilled, use t-test.

## 5.2 Hypthesis testing for one sample

- One-tail hypothesis test: Used, when interested in whether the value of the parameter of interest is less the value specified in the null hypothesis *OR* parameter of interest is greater the value specified in the null hypothesis (NOT BOTH). $H-0: \mu = 0$ versus $H_a: \mu > 0$ *OR* $H_a: \mu < 0$
- Two-tail hypothesis test: Used, when interested in whether the value of the parameter of interest differs (either above or below) from the value specified in the null hypothesis. $H_0: \mu = 1000$, $H_a: \mu \neq 1000$

### Applying a z-test for one sample

```{r}
# The aim of this example is to test the hypothesis that people working in profession 7 have an IQ higher than average, at a significance level of 5%. Note that IQ test scores are calculated based on a normed group with average 100 and a standard deviation 15. 
iq_df <- read.csv(here("ZDatasets","IQ.csv"))
head(iq_df)
```

```{r}
# Subset profession 7
profession7 <- iq_df |> filter(Profession == 7)
# Check the sample size
profession7$IQ |> length()
```

Since the population SD is known and the sample size is large, a z-test is conducted.

```{r}
population_sd <- 15
population_mu <- 100
n <- profession7$IQ |> length()
x_bar <- mean(profession7$IQ)
SE <- population_sd / sqrt(n)
z_score <- (x_bar - population_mu) / SE
sig_level <- 0.05

# Upper tail test has p-value = P(Z > z)
p_value = pnorm(z_score, lower.tail=FALSE)
cat(paste("p-value: ", p_value))
```

Based on the p-value and the significance level (0.05), we can conclude our findings.

```{r}
statement = c("the null hypothesis at the 5% significance level.",
"\nConclude that those working in profession 7 have")
true = c("Reject", statement, "\na higher average IQ than the overall population.")
false = c("Fail to reject",statement, "\nthe same IQ as the overall population.")
if (p_value < sig_level) cat(paste(true)) else cat(paste(false))
```

### Applying a t-test for one sample

- If sample size is small (n < 31)
- If population SD is unknown

The procedure is nearly identical, we just calculate the t-value and degrees of freedom (df = n-1).

- `pt(t_score, df)` # For one-tail test, smaller than P(t < t_score)
- `1 - pt(t_score, df)` # For one-tail test, greater than P(t > t_score)

#### Example

We have some diameter data (in cm) for a motorcycle brake disc manufacturer. The manufacturer claims their brake diameter is in keeping with the regulation diameter of 12 inches (30.48 cm) for this part. Conduct a two tailed test at 5% significance level to decide whether we should reject the manufacturer’s claim.

$$H_0: \mu = 30.48$$
$$H_a: \mu \neq 30.48$$
```{r, messages=FALSE}
# Example
brakes_df <- read_csv(here("ZDatasets","new_disc_brakes.csv"))
brakes_df |> head()
```

Check the length of brakes data.
```{r}
length(brakes_df$brake_diameter)
```

Hence the dataset is small and the population standard deviation was not provided, we will require a t-test. However, due to our sample size we must check to see whether our sample data exhibits a normal distribution.

```{r}
# Assess normality
ggplot(brakes_df,aes(x=brake_diameter)) +
  geom_histogram(bins = 7, color = 'white', fill='steelblue') +
  labs(x = "x", y = "Frequency", title = "Histogram of brake disc diameter (cm)") +
  geom_vline(xintercept = 30.48, color = "red")
```

The data looks normality distributed, more test are usually required (Shapiro-Wilk-test).

```{r}
# Now that we can assume normality, we can perform a t-test
t.test(brakes_df$brake_diameter, alternative = "two.sided", mu = 30.48)$p.value

```

Our p-value of 0.017 suggests we reject the manufacturer's claim. p-value (0.017) < significance level 0.05.


## 5.3 Hypothesis testing for two samples

This time it is the differences between the group means or proportions that are in focus. You will need to check whether the samples are drawn from two independent (unrelated) populations, or from two dependent (related).

### 5.3.1 Hypothesis testing for the means of two samples

The following conditions need to be met for large samples n > 30:
- Both datasets have been chosen based on simple random sampling
- Both samples are large

The following conditions need to be met for small samples n < 31:
- Both datasets have been chosen based on simple random sampling
- One or both samples are small
- Both populations are normally distributed

#### 5.3.1.1 Checking the assumption of normality and equal variances

```{r}
# Get the data
rareetals_df <- read_csv(here("Zdatasets","rare_metal_concentrations.csv"))
head(rareetals_df,3)
```

Our hypothesis are:

$$H_0: \mu_{\text{Location1}} = \mu_{\text{Location2}}$$
$$H_0: \mu_{\text{Location1}} \neq \mu_{\text{Location2}}$$

```{r}
# Summary statistics
stats_Location1 <- summarise(rareetals_df,
                             mean = mean(rareetals_df$Location.1, na.rm=TRUE),
                             sd = sd(rareetals_df$Location.1, na.rm = TRUE),
                             n = sum(!is.na(rareetals_df$Location.1)))
stats_Location1
```

```{r}
# Summary statistics
stats_Location2 <- summarise(rareetals_df,
                             mean = mean(rareetals_df$Location.2),
                             sd = sd(rareetals_df$Location.2),
                             n = sum(!is.na(rareetals_df$Location.2)))
stats_Location2
```

Check the assumption of normality using Shapiro-Wilk test:

The null hypothesis of this test is that the data is normally distributed. The alternative is that the data has a non-normal distribution. If p-value < 0.05 then reject normality. Otherwise do not reject if the p-value > 0.05.

```{r}
p_val_1 <- shapiro.test(rareetals_df$Location.1)
p_val_2 <- shapiro.test(rareetals_df$Location.2)
# Print the result
cat(paste("p-value for testing normality of Location 1 = ", p_val_1$p.value,'\n'))
cat(paste("p-value for testing normality of Location 2 = ", p_val_2$p.value))
```

Check the assumption of equal variances

```{r}
ratio <- stats_Location1$sd / stats_Location2$sd
ratio
```

As the ratio belongs to the interval (-0.5,2) we assume equal variances.

Two samples t-test:

Now since all assumptions are met:

```{r}
t.test(rareetals_df$Location.1, rareetals_df$Location.2,
       alternative = "two.sided", paired = FALSE, var.equal = TRUE)
```

Since p-value (0.062) > significance level (0.05) you fail to reject the hypothesis of equal means and conclude that there is no significant difference between the rare metal concentrations at the two locations.

### 5.3.2 Comparing two proportions with large samples

























